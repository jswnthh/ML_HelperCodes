#1 
def check_df(dataframe, head=5):
    print(" SHAPE ".center(70,'-'))
    print('Rows: {}'.format(dataframe.shape[0]))
    print('Columns: {}'.format(dataframe.shape[1]))
    print(" TYPES ".center(70,'-'))
    print(dataframe.dtypes)
    print(" HEAD ".center(70,'-'))
    print(dataframe.head(head))
    print(" TAIL ".center(70,'-'))
    print(dataframe.tail(head))
    print(" MISSING VALUES ".center(70,'-'))
    print(dataframe.isnull().sum())
    print(" DUPLICATED VALUES ".center(70,'-'))
    print(dataframe.duplicated().sum())
    print(" DESCRIBE ".center(70,'-'))
    print(dataframe.describe([0, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1]).T)
    
check_df(df)

#2 
import seaborn as sns
sns.set(style='darkgrid',
        palette='Set2',
        font_scale=1,
        font='calibri',
        rc={'figure.figsize':(8,6), "legend.fontsize": 14})

sns.set_context("paper", rc={
    "lines.linewidth": 1.5,
    "axes.labelsize": 14,
    "xtick.labelsize": 12,
    "ytick.labelsize": 12
})

sns.set_style({'grid.color': '.95', 'axes.edgecolor': '.3'})

#3 SEPARATE COLUMS AS NUMERICAL AND CATEGORICAL
-----------------------------------------------
def separate_num_cat(df):
    numerical_values = {}
    categorical_values = {}
    
    for column in df.columns:
        if df[column].dtype == 'object':
            categorical_values[column] = df[column].dtype
        else:
            numerical_values[column] = df[column].dtype
    
    print("-" * 70)
    print(" Numerical Values ".center(70))
    print("-" * 70)
    print("\n")
    for column, dtype in numerical_values.items():
        print(f"{column}: {dtype}")
    print("\n")
    print("-" * 70)
    print(" Categorical Values ".center(70))
    print("-" * 70)
    print("\n")
    for column, dtype in categorical_values.items():
        print(f"{column}: {dtype}")

    
    #4 DOWNCASTING DATATYPE
    -----------------------
    def reduce_mem_usage(df):
    """
    Iterate through all the columns of a dataframe and modify the data type
    to reduce memory usage.
    """
    # Calculate the initial memory usage of the DataFrame
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of the dataframe is {:.2f} MB'.format(start_mem))
    
    # Iterate through each column of the DataFrame
    for column in df.columns:
        # Get the current data type of the column
        col_type = df[column].dtype
        
        # Check if the column is not an object (i.e., numeric)
        if col_type != object:
            # Get the minimum and maximum values of the column
            c_min = df[column].min()
            c_max = df[column].max()
            
            # Check if the data type is integer
            if str(col_type)[:3] == 'int':
                # Downcast the column to the smallest integer type if its values are within the range
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[column] = df[column].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[column] = df[column].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[column] = df[column].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[column] = df[column].astype(np.int64)
            # Check if the data type is floating-point
            else:
                # Downcast the column to the smallest floating-point type if its values are within the range
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[column] = df[column].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[column] = df[column].astype(np.float32)
                else:
                    df[column] = df[column].astype(np.float64)
        # Convert object columns to categorical type
        else:
            df[column] = df[column].astype('category')

    # Calculate the final memory usage of the DataFrame
    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df
    
#5 A HELPER FUNCTION TO PLOT 3 KINDS OF PLOTS 
- HISTPLOT
- BOXPLOT
- VIOLINPLOT
---------------------------------------------
import seaborn as sns
import matplotlib.pyplot as plt
import math

def plot(data, plot_type, *features):
    sns.set(style='darkgrid')
    num_plots = len(features)
    num_cols = math.ceil(math.sqrt(num_plots))
    num_rows = math.ceil(num_plots / num_cols)

    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))
    axes = axes.flatten()

    for i, feature in enumerate(features):
        ax = axes[i]
        if plot_type == 'histplot':
            sns.histplot(data=data, x=feature, bins=10, ax=ax)
        elif plot_type == 'boxplot':
            sns.boxplot(data=data, x=feature, ax=ax)
        elif plot_type == 'violinplot':
            sns.violinplot(data=data, x=feature, ax=ax)
        ax.set_title(feature)

    # Remove any unused subplots
    if len(features) < len(axes):
        for j in range(len(features), len(axes)):
            fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()
    
    
#6 RECEIVER OPERATING CHARATERIC CURVE FOR GIVEN MODEL'S PREDICTION
-------------------------------------------------------------------


def test_roc(model, data, labels):
    if hasattr(model, "decision_function"):
        predictions = model.decision_function(data)
    else:
        predictions = model.predict_proba(data)[:,1]
    fpr, tpr, _ = sklearn.metrics.roc_curve(labels, predictions)
    return fpr, tpr
    
    
#7 INERTIA GRAPH AND HEATMAP
---------------------------------------
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Generate sample data (you can replace this with your own data)
X = df["Interest_Rate"].array.reshape(-1, 1)

# Define the range of k values to test
k_values = range(1, 10)

# Initialize an empty list to store the inertia values
inertia_values = []

# Perform K-means clustering for each value of k
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertia_values.append(kmeans.inertia_)

# Set up the figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Plot the inertia graph on the first subplot
axes[0].plot(k_values, inertia_values, marker='o')
axes[0].set_xlabel('Number of Clusters (k)')
axes[0].set_ylabel('Inertia')
axes[0].set_title('Inertia Graph')

# Perform K-means clustering with the optimal k value
optimal_k = 3  # Replace with your optimal k value
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
y_kmeans = kmeans.fit_predict(X)

# Plot the clustermap on the second subplot
cluster_labels = kmeans.labels_
cluster_data = pd.DataFrame({'Interest Rate': df['Interest_Rate'], 'Cluster Label': cluster_labels})
clustermap_data = cluster_data.pivot_table(index='Cluster Label', columns='Interest Rate', aggfunc='size', fill_value=0)
sns.heatmap(clustermap_data, cmap='Blues', ax=axes[1])
axes[1].set_xlabel('Interest Rate')
axes[1].set_ylabel('Cluster Label')
axes[1].set_title('Cluster Map')

# Adjust the spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()
# Adjust the spacing between subplots
plt.tight_layout()

# Display the plotplt.show()

